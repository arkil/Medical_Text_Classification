{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import operator\n",
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.dat', sep='\\t',header=None, names = [\"label\",\"extract\"])\n",
    "test_data = pd.read_csv('test.dat', sep='\\t',header=None, names = [\"extract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Catheterization laboratory events and hospital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Renal abscess in children. Three cases of rena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hyperplastic polyps seen at sigmoidoscopy are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Subclavian artery to innominate vein fistula a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Effect of local inhibition of gamma-aminobutyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Infection during chronic epidural catheterizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Mediastinal tracheostomy using a pectoralis ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Tumefactive fibroinflammatory lesion of the ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Multiple representations contribute to body kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Increasing asthma prevalence in a rural New Ze...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            extract\n",
       "0      4  Catheterization laboratory events and hospital...\n",
       "1      5  Renal abscess in children. Three cases of rena...\n",
       "2      2  Hyperplastic polyps seen at sigmoidoscopy are ...\n",
       "3      5  Subclavian artery to innominate vein fistula a...\n",
       "4      4  Effect of local inhibition of gamma-aminobutyr...\n",
       "5      1  Infection during chronic epidural catheterizat...\n",
       "6      5  Mediastinal tracheostomy using a pectoralis ma...\n",
       "7      5  Tumefactive fibroinflammatory lesion of the ex...\n",
       "8      3  Multiple representations contribute to body kn...\n",
       "9      5  Increasing asthma prevalence in a rural New Ze..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excision of limbal dermoids. We reviewed the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bell's palsy. A diagnosis of exclusion. In cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Retained endobronchial foreign body removal fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recurrent buccal space abscesses: a complicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intracranial fibromatosis. Fibromatoses are un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The effect of intrathecal morphine on somatose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The 29th Rovenstine lecture: clinical challeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mortality in patients treated with flecainide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Approaches to immunotherapy of cancer: charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Retinal artery obstruction and atheromas assoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             extract\n",
       "0  Excision of limbal dermoids. We reviewed the c...\n",
       "1  Bell's palsy. A diagnosis of exclusion. In cas...\n",
       "2  Retained endobronchial foreign body removal fa...\n",
       "3  Recurrent buccal space abscesses: a complicati...\n",
       "4  Intracranial fibromatosis. Fibromatoses are un...\n",
       "5  The effect of intrathecal morphine on somatose...\n",
       "6  The 29th Rovenstine lecture: clinical challeng...\n",
       "7  Mortality in patients treated with flecainide ...\n",
       "8  Approaches to immunotherapy of cancer: charact...\n",
       "9  Retinal artery obstruction and atheromas assoc..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.335296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.552432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  14438.000000\n",
       "mean       3.335296\n",
       "std        1.552432\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Long-term results after lateral cranial base s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  extract\n",
       "count                                               14442\n",
       "unique                                              11279\n",
       "top     Long-term results after lateral cranial base s...\n",
       "freq                                                    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = train_data[\"extract\"]\n",
    "train_docs = [l.split() for l in extracted_data]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "minlen =  4\n",
    "\n",
    "def preprocess_doc(docs):\n",
    "    for ind1,doc in enumerate(docs):\n",
    "        for ind2,word in enumerate(doc):\n",
    "            if (len(word) >= minlen) and (word not in stop_words) and word.isalnum():\n",
    "                docs[ind1][ind2] = word.lower()\n",
    "#                 ls = LancasterStemmer()\n",
    "#                 docs[ind1][ind2] = ls.stem(word)\n",
    "                lm = WordNetLemmatizer()\n",
    "                docs[ind1][ind2] = lm.lemmatize(docs[ind1][ind2])\n",
    "            else:\n",
    "                del docs[ind1][ind2]\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "# def filter_Len(docs, minlen):\n",
    "#     return [ [t.lower() for t in d if len(t) >= minlen] for d in docs ]\n",
    "\n",
    "# def stop_filter(docs):\n",
    "#     return [ [t for t in d if t not in stop_words ] for d in docs ]\n",
    "\n",
    "# def stem_words(docs):\n",
    "#     for ind1,doc in enumerate(docs):\n",
    "#         for ind2,word in enumerate(doc):\n",
    "#             ls = LancasterStemmer()\n",
    "#             docs[ind1][ind2] = ls.stem(word)\n",
    "#     return docs\n",
    "\n",
    "# def lemmatize_words(docs):\n",
    "#     for ind1,doc in enumerate(docs):\n",
    "#         for ind2,word in enumerate(doc):\n",
    "#             lm = WordNetLemmatizer()\n",
    "#             docs[ind1][ind2] = lm.lemmatize(word)\n",
    "#     return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #alpha_num_docs = remove_special_characters(filtered_docs)\n",
    "# stop_filtered_docs = stop_filter(docs)\n",
    "# filtered_docs = filter_Len(stop_filtered_docs,4)\n",
    "# stemed_docs = stem_words(filtered_docs)\n",
    "# lemm_docs = lemmatize_words(stemed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(docs[0]), docs[0][:20])\n",
    "# print(len(filtered_docs[0]), filtered_docs[0][:20])\n",
    "# print(len(stop_filtered_docs[0]), stop_filtered_docs[0][:20])\n",
    "# print(len(stemed_docs[0]), stemed_docs[0][:20])\n",
    "# print(len(lemm_docs[0]), lemm_docs[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_doc =preprocess_doc(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 ['catheterization', 'laboratory', 'event', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'catheterization', 'laboratory']\n",
      "155 ['catheterization', 'laboratory', 'event', 'hospital', 'outcome', 'direct', 'angioplasty', 'acute', 'myocardial', 'infarction', 'assess', 'safety', 'direct', 'infarct', 'angioplasty', 'without', 'antecedent', 'thrombolytic', 'catheterization', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs[0]), train_docs[0][:20])\n",
    "print(len(preprocessed_doc[0]), preprocessed_doc[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_test_data = test_data[\"extract\"]\n",
    "test_docs = [l.split() for l in extracted_test_data]\n",
    "\n",
    "\n",
    "# stop_filtered_test_docs = stop_filter(test_docs)\n",
    "# filtered_test_docs = filter_Len(stop_filtered_test_docs,3)\n",
    "# stemed_test_docs = stem_words(filtered_test_docs)\n",
    "# lemm__test_docs = lemmatize_words(stemed_test_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_doc =preprocess_doc(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 ['excision', 'limbal', 'We', 'reviewed', 'clinical', 'file', '10', 'patient', 'had', 'undergone', 'excision', 'unilateral', 'epibulbar', 'limbal', 'Preoperatively,', 'of', 'affected', 'eye', 'worse', 'visual']\n",
      "78 ['excision', 'limbal', 'We', 'reviewed', 'clinical', 'file', '10', 'patient', 'had', 'undergone', 'excision', 'unilateral', 'epibulbar', 'limbal', 'Preoperatively,', 'of', 'affected', 'eye', 'worse', 'visual']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_docs[0]), test_docs[0][:20])\n",
    "print(len(preprocessed_test_doc[0]), preprocessed_test_doc[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    5\n",
       "2    2\n",
       "3    5\n",
       "4    4\n",
       "5    1\n",
       "6    5\n",
       "7    5\n",
       "8    3\n",
       "9    5\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = train_data[\"label\"]\n",
    "cls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat, idx\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 14438, ncols 70735, nnz 1161420]\n"
     ]
    }
   ],
   "source": [
    "#mat, idx = build_matrix(train_docs)\n",
    "train_mat, idx = build_matrix(preprocessed_doc)\n",
    "#csr_info(mat)\n",
    "csr_info(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix_test(docs,idx):\n",
    "    nrows = len(docs)\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            temp = idx.get(k,-1)\n",
    "            if temp != -1:\n",
    "                ind[j+n] = temp\n",
    "                val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 14442, ncols 70735, nnz 1182300]\n"
     ]
    }
   ],
   "source": [
    "#test_mat = build_matrix_test(test_docs,idx)\n",
    "test_mat = build_matrix_test(preprocessed_test_doc,idx)\n",
    "#csr_info(test_mat)\n",
    "csr_info(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5.]] \n",
      "\n",
      "mat1: (14438, 70735) \n",
      "\n",
      "mat1: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.         10.86061618  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          3.00931134]] \n",
      "\n",
      "mat1: (14438, 70735) \n",
      "\n",
      "mat1: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15118496 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.04189105]] \n",
      "\n",
      "mat1: (14438, 70735) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mat1 = csr_idf(train_mat, copy=True)\n",
    "train_mat2= csr_l2normalize(train_mat1, copy=True)\n",
    "print(\"mat1:\", train_mat[12,:20].todense(), \"\\n\")\n",
    "print(\"mat1:\", train_mat.shape, \"\\n\")\n",
    "\n",
    "print(\"mat1:\", train_mat1[12,:20].todense(), \"\\n\")\n",
    "print(\"mat1:\", train_mat1.shape, \"\\n\")\n",
    "\n",
    "print(\"mat1:\", train_mat2[12,:20].todense(), \"\\n\")\n",
    "print(\"mat1:\", train_mat2.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2.]] \n",
      "\n",
      "mat2: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  2.07460628 1.20023662]] \n",
      "\n",
      "mat3: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0380577  0.02201779]]\n"
     ]
    }
   ],
   "source": [
    "test_mat1 = csr_idf(test_mat, copy=True)\n",
    "test_mat2 = csr_l2normalize(test_mat1, copy=True)\n",
    "print(\"mat1:\", test_mat[15,:20].todense(), \"\\n\")\n",
    "print(\"mat2:\", test_mat1[15,:20].todense(), \"\\n\")\n",
    "print(\"mat3:\", test_mat2[15,:20].todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(matrix_a,matrix_b):\n",
    "    cosine_sim = matrix_a*(matrix_b.T)\n",
    "    return cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classlabel_voting(cos_sim_list, k, epsilon):\n",
    "        k_neighbours = cos_sim_list[:k]\n",
    "        class_label_dict = {}\n",
    "        for neighbour in k_neighbours:\n",
    "            classlabel = neighbour[0]\n",
    "            sim_measure = neighbour[1]\n",
    "            if sim_measure < epsilon:\n",
    "                continue\n",
    "            if classlabel in class_label_dict:\n",
    "                class_label_dict[classlabel] += 1\n",
    "            else :\n",
    "                class_label_dict[classlabel] = 1\n",
    "        sorted_class_labels_list = sorted(class_label_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        if len(sorted_class_labels_list) > 0:\n",
    "            req_class_label = sorted_class_labels_list[0][0]\n",
    "        else:\n",
    "            req_class_label = k_neighbours[0][0]\n",
    "        return req_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algorithm(train_matrix, test_matrix, class_labels, k, epsilon):\n",
    "        nrows = test_matrix.shape[0]\n",
    "        predictions_file = open(\"smsn.dat\",\"w\")\n",
    "        for i in range(nrows):\n",
    "            similarity_matrix = cosine_similarity(train_matrix, test_matrix[i])\n",
    "#             print(similarity_matrix)\n",
    "            sim_map = {}\n",
    "            nrows_sim_matrix = similarity_matrix.shape[0]\n",
    "            for j in range(nrows_sim_matrix):\n",
    "                sim_map[class_labels[j]] = similarity_matrix[j]             \n",
    "            sorted_sim_list = sorted(sim_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            res_class_label = classlabel_voting(sorted_sim_list, k, epsilon)\n",
    "            predictions_file.write(str(res_class_label))\n",
    "            predictions_file.write(\"\\n\")\n",
    "        predictions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "epsilon = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rutul Thakkar\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:274: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using < is inefficient, try using >= instead.\n",
      "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "knn_algorithm(train_mat2, test_mat2, cls, k, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_similarity(testmat,trainmat):       \n",
    "#         cosine_sim_mat = trainmat.dot(testmat.T)\n",
    "#         return cosine_sim_mat\n",
    "\n",
    "# knn = 3\n",
    "# epsilon =0.1\n",
    "# #%%\n",
    "# def knn_algorithm(train_matrix, test_matrix, class_labels_arr, knn, epsilon):\n",
    "#         pred_format_file = open(\"myfinaltestprg1pred.dat\",\"w\") \n",
    "#         cos_sim_matrix = cosine_similarity(test_matrix, train_matrix)    \n",
    "#         ##rows of sim matrix is same as number of rows of trainingset and cols are same as number of rows of test set\n",
    "#         ncols_sim_mat = cos_sim_matrix.shape[1] # this is same as number of rows in test matrix   \n",
    "#         for i in range(ncols_sim_mat):\n",
    "#             #get first column values\n",
    "#             row_sim_array = cos_sim_matrix[:,i].toarray()\n",
    "#             npsimarr = numpy.array(row_sim_array.copy())          \n",
    "#             dict1_labels = {}\n",
    "#             for i in range(knn):\n",
    "#                 maxval = numpy.amax(npsimarr, axis=0)\n",
    "#                 #print(maxval[0],end=\",\")\n",
    "#                 if maxval[0] < epsilon:\n",
    "#                     continue\n",
    "#                 indexarr = numpy.where(npsimarr == numpy.amax(npsimarr))\n",
    "#                 index = indexarr[0][0]\n",
    "#                 #print(index,end=\",\")\n",
    "#                 npsimarr[index][0] = 0.0\n",
    "#                 classlabel = class_labels_arr[index]\n",
    "#                 if classlabel in dict1_labels:\n",
    "#                     dict1_labels[classlabel] += 1\n",
    "#                 else:\n",
    "#                     dict1_labels[classlabel] = 1\n",
    "#             #print(dict1_labels) \n",
    "#             sorted_labels = sorted(dict1_labels.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#             #print(sorted_labels)\n",
    "#             finallabel = sorted_labels[0][0]\n",
    "#             print(finallabel)\n",
    "#             pred_format_file.write(finallabel)           \n",
    "#             pred_format_file.write(\"\\n\")            \n",
    "#         pred_format_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
